{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33cec736",
   "metadata": {},
   "source": [
    "\n",
    "# Transformer-based English–French Translation\n",
    "\n",
    "Notebook ini merupakan implementasi eksplorasi model **Transformer** untuk penerjemahan otomatis dari Bahasa Inggris ke Bahasa Prancis.\n",
    "\n",
    "Tujuan utama:\n",
    "- Melatih model Transformer selama **1 epoch** dengan **batch-size maksimal 100**\n",
    "- Menunjukkan proses **Text Preprocessing**, **Definisi Arsitektur Transformer**, **Training**, dan **Inference**\n",
    "- Menampilkan metrik: `TrainLoss`, `ValLoss`, dan `ValAcc` di setiap akhir batch.\n",
    "\n",
    "Bobot penilaian:\n",
    "| Aspek | Bobot |\n",
    "|-------|-------|\n",
    "| Data Preparation (Text Preprocessing) | 20% |\n",
    "| Definisi Class Transformer | 25% |\n",
    "| Proses Training (TrainLoss, ValLoss, ValAcc) | 35% |\n",
    "| Inference Translation | 20% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a108dd3",
   "metadata": {},
   "source": [
    "## Persiapan Lingkungan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74556571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Running on cuda\n",
      "Running on cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\app\\envs\\steganalysis\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: pandas in d:\\app\\envs\\steganalysis\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in d:\\app\\envs\\steganalysis\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: filelock in d:\\app\\envs\\steganalysis\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\app\\envs\\steganalysis\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\app\\envs\\steganalysis\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\app\\envs\\steganalysis\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\app\\envs\\steganalysis\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\app\\envs\\steganalysis\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\app\\envs\\steganalysis\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\app\\envs\\steganalysis\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\app\\envs\\steganalysis\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\app\\envs\\steganalysis\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\app\\envs\\steganalysis\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\app\\envs\\steganalysis\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install torch pandas numpy\n",
    "import torch, pandas as pd, numpy as np, random, math, re, os\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on', DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c85ef7",
   "metadata": {},
   "source": [
    "## 1. Data Preparation & Text Preprocessing (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf55fa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab sizes -> src: 231 | tgt: 358\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dataset diambil dari file small_vocab_en.csv dan small_vocab_fr.csv\n",
    "# Gunakan path relatif terhadap working directory agar berjalan di Windows/Local\n",
    "en_path = os.path.join(os.getcwd(), 'small_vocab_en.csv')\n",
    "fr_path = os.path.join(os.getcwd(), 'small_vocab_fr.csv')\n",
    "\n",
    "# Pastikan file ada - bila tidak ada, munculkan pesan informatif\n",
    "if not os.path.exists(en_path) or not os.path.exists(fr_path):\n",
    "    raise FileNotFoundError(f'Required files not found. Expected at: {en_path} and {fr_path}')\n",
    "\n",
    "# Beberapa kalimat mengandung koma; baca tiap baris langsung dari file untuk menghindari parser CSV\n",
    "with open(en_path, 'r', encoding='utf-8') as f:\n",
    "    src_texts = [line.strip() for line in f if line.strip()]\n",
    "with open(fr_path, 'r', encoding='utf-8') as f:\n",
    "    tgt_texts = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zâêôàèçùé'\\-\\.\\,\\?\\!\\s]\", ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def tokenize(text): return text.split()\n",
    "\n",
    "src_tokens = [tokenize(clean_text(s)) for s in src_texts]\n",
    "tgt_tokens = [tokenize(clean_text(t)) for t in tgt_texts]\n",
    "\n",
    "# Split train/val\n",
    "data = list(zip(src_tokens, tgt_tokens))\n",
    "random.shuffle(data)\n",
    "# Buat split train/val, pastikan val tidak kosong (jika dataset sangat kecil)\n",
    "split = int(0.9 * len(data))\n",
    "if split >= len(data):\n",
    "    split = max(1, len(data) - 1)\n",
    "train, val = data[:split], data[split:]\n",
    "\n",
    "PAD, BOS, EOS, UNK = '<pad>', '<s>', '</s>', '<unk>'\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    counter = Counter(t for s in sentences for t in s)\n",
    "    vocab = [PAD, BOS, EOS, UNK] + [t for t, _ in counter.most_common()]\n",
    "    stoi = {t: i for i, t in enumerate(vocab)}\n",
    "    itos = {i: t for t, i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "src_stoi, src_itos = build_vocab([s for s, _ in train])\n",
    "tgt_stoi, tgt_itos = build_vocab([t for _, t in train])\n",
    "\n",
    "print('Vocab sizes -> src:', len(src_stoi), '| tgt:', len(tgt_stoi))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fede71",
   "metadata": {},
   "source": [
    "## 2. Definisi Arsitektur Transformer (25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb512b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TransformerMT(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, d_model=256, nhead=8, nlayers=3):\n",
    "        super().__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab, d_model)\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab, d_model)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        self.trans = nn.Transformer(d_model, nhead, nlayers, nlayers, batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.pos(self.src_emb(src) * math.sqrt(self.d_model))\n",
    "        tgt = self.pos(self.tgt_emb(tgt) * math.sqrt(self.d_model))\n",
    "        mask = torch.tril(torch.ones(tgt.size(1), tgt.size(1), device=DEVICE)).bool()\n",
    "        out = self.trans(src, tgt, tgt_mask=mask)\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dfe426",
   "metadata": {},
   "source": [
    "## 3. Proses Training (35%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac9a23e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 | TrainLoss 6.0151 | ValLoss nan | ValAcc 0.0000\n",
      "Batch 2 | TrainLoss 5.4421 | ValLoss nan | ValAcc 0.0000\n",
      "Batch 2 | TrainLoss 5.4421 | ValLoss nan | ValAcc 0.0000\n",
      "Batch 3 | TrainLoss 5.1770 | ValLoss nan | ValAcc 0.0000\n",
      "Batch 3 | TrainLoss 5.1770 | ValLoss nan | ValAcc 0.0000\n",
      "Batch 4 | TrainLoss 5.0295 | ValLoss nan | ValAcc 0.0000\n",
      "Batch 4 | TrainLoss 5.0295 | ValLoss nan | ValAcc 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), t_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(); opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 44\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | TrainLoss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | ValLoss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | ValAcc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, t_in, t_out \u001b[38;5;129;01min\u001b[39;00m val_dl:\n\u001b[0;32m     29\u001b[0m     s,t_in,t_out \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mto(DEVICE),t_in\u001b[38;5;241m.\u001b[39mto(DEVICE),t_out\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 30\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), t_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     32\u001b[0m     pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\app\\envs\\steganalysis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\app\\envs\\steganalysis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m, in \u001b[0;36mTransformerMT.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     26\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_emb(tgt) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model))\n\u001b[0;32m     27\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones(tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39mDEVICE))\u001b[38;5;241m.\u001b[39mbool()\n\u001b[1;32m---> 28\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrans\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n",
      "File \u001b[1;32md:\\app\\envs\\steganalysis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\app\\envs\\steganalysis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\app\\envs\\steganalysis\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:276\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    271\u001b[0m     src,\n\u001b[0;32m    272\u001b[0m     mask\u001b[38;5;241m=\u001b[39msrc_mask,\n\u001b[0;32m    273\u001b[0m     src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[0;32m    274\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal,\n\u001b[0;32m    275\u001b[0m )\n\u001b[1;32m--> 276\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32md:\\app\\envs\\steganalysis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\app\\envs\\steganalysis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\app\\envs\\steganalysis\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:607\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    604\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[0;32m    606\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m _get_seq_len(tgt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m--> 607\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m \u001b[43m_detect_is_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m    610\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[0;32m    611\u001b[0m         output,\n\u001b[0;32m    612\u001b[0m         memory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m         memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal,\n\u001b[0;32m    619\u001b[0m     )\n",
      "File \u001b[1;32md:\\app\\envs\\steganalysis\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:1206\u001b[0m, in \u001b[0;36m_detect_is_causal_mask\u001b[1;34m(mask, is_causal, size)\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;66;03m# Do not use `torch.equal` so we handle batched masks by\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;66;03m# broadcasting the comparison.\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m causal_comparison\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 1206\u001b[0m     make_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_comparison\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1208\u001b[0m     make_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_stoi, tgt_stoi, max_len=50):\n",
    "        self.data = pairs; self.src_stoi = src_stoi; self.tgt_stoi = tgt_stoi; self.max_len = max_len\n",
    "\n",
    "    def encode(self, tokens, vocab):\n",
    "        ids = [vocab.get(t, vocab['<unk>']) for t in tokens] + [vocab['</s>']]\n",
    "        return ids + [vocab['<pad>']] * (self.max_len - len(ids)) if len(ids)<self.max_len else ids[:self.max_len]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s, t = self.data[idx]\n",
    "        src = torch.tensor(self.encode(s, self.src_stoi))\n",
    "        tgt = torch.tensor([self.tgt_stoi['<s>']] + self.encode(t, self.tgt_stoi))\n",
    "        return src, tgt[:-1], tgt[1:]\n",
    "    def __len__(self): return len(self.data)\n",
    "\n",
    "train_ds = TranslationDataset(train, src_stoi, tgt_stoi)\n",
    "val_ds = TranslationDataset(val, src_stoi, tgt_stoi)\n",
    "train_dl = DataLoader(train_ds, batch_size=100, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=100)\n",
    "\n",
    "model = TransformerMT(len(src_stoi), len(tgt_stoi)).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_stoi['<pad>'])\n",
    "\n",
    "def evaluate():\n",
    "    model.eval(); loss, correct, total = 0,0,0\n",
    "    with torch.no_grad():\n",
    "        for s, t_in, t_out in val_dl:\n",
    "            s,t_in,t_out = s.to(DEVICE),t_in.to(DEVICE),t_out.to(DEVICE)\n",
    "            out = model(s,t_in)\n",
    "            loss += loss_fn(out.reshape(-1,out.size(-1)), t_out.reshape(-1)).item()\n",
    "            pred = out.argmax(-1)\n",
    "            mask = t_out!=tgt_stoi['<pad>']\n",
    "            correct += (pred==t_out).masked_select(mask).sum().item()\n",
    "            total += mask.sum().item()\n",
    "    return loss/len(val_dl), correct/total\n",
    "\n",
    "for b,(s,t_in,t_out) in enumerate(train_dl,1):\n",
    "    model.train(); s,t_in,t_out=s.to(DEVICE),t_in.to(DEVICE),t_out.to(DEVICE)\n",
    "    opt.zero_grad()\n",
    "    out = model(s,t_in)\n",
    "    loss = loss_fn(out.reshape(-1,out.size(-1)), t_out.reshape(-1))\n",
    "    loss.backward(); opt.step()\n",
    "    val_loss, val_acc = evaluate()\n",
    "    print(f'Batch {b} | TrainLoss {loss.item():.4f} | ValLoss {val_loss:.4f} | ValAcc {val_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36f687",
   "metadata": {},
   "source": [
    "## 4. Inference Translation (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb855b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate(sentence_tokens):\n",
    "    model.eval()\n",
    "    src = torch.tensor([train_ds.encode(sentence_tokens, src_stoi)]).to(DEVICE)\n",
    "    tgt = torch.tensor([[tgt_stoi['<s>']]]).to(DEVICE)\n",
    "    for _ in range(50):\n",
    "        out = model(src, tgt)\n",
    "        next_tok = out[0,-1].argmax().item()\n",
    "        tgt = torch.cat([tgt, torch.tensor([[next_tok]], device=DEVICE)], dim=1)\n",
    "        if next_tok == tgt_stoi['</s>']: break\n",
    "    return ' '.join(tgt_itos[i.item()] for i in tgt[0][1:-1])\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC :', ' '.join(val[i][0]))\n",
    "    print('PRED:', translate(val[i][0]))\n",
    "    print('REF :', ' '.join(val[i][1]))\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582065aa",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Kesimpulan\n",
    "\n",
    "Eksperimen ini menunjukkan implementasi dasar Transformer untuk penerjemahan Bahasa Inggris ke Bahasa Prancis.\n",
    "\n",
    "- Data telah dibersihkan dan ditokenisasi secara sederhana.\n",
    "- Arsitektur Transformer telah dibangun dari nol dengan PyTorch.\n",
    "- Proses training menampilkan *TrainLoss*, *ValLoss*, dan *ValAcc* tiap batch.\n",
    "- Model berhasil melakukan inferensi dengan pendekatan *greedy decoding*.\n",
    "\n",
    "Selanjutnya, model dapat diperluas dengan peningkatan jumlah epoch, mekanisme perhatian visualisasi, dan evaluasi BLEU score.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steganalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
