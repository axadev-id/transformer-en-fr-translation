{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d363b67d",
      "metadata": {},
      "outputs": [],
      "source": [
        "    # src padding mask (batch_size=1)\n",
        "    src_key_padding_mask = (src_ids == src_stoi['<pad>'])\n",
        "    src_key_padding_mask = (src_ids == src_stoi['<pad>'])\n",
        "    src_key_padding_mask = (src_ids == src_stoi['<pad>'])\n",
        "{\n",
        "    \"cells\": [\n",
        "        {\n",
        "            \"cell_type\": \"markdown\",\n",
        "            \"id\": \"#VSC-1b097b09\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"markdown\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"\",\n",
        "                \"# Transformer-based English–French Translation\",\n",
        "                \"\",\n",
        "                \"Notebook ini merupakan implementasi eksplorasi model **Transformer** untuk penerjemahan otomatis dari Bahasa Inggris ke Bahasa Prancis.\",\n",
        "                \"\",\n",
        "                \"Tujuan utama:\",\n",
        "                \"- Melatih model Transformer selama **1 epoch** dengan **batch-size maksimal 100**\",\n",
        "                \"- Menunjukkan proses **Text Preprocessing**, **Definisi Arsitektur Transformer**, **Training**, dan **Inference**\",\n",
        "                \"- Menampilkan metrik: `TrainLoss`, `ValLoss`, dan `ValAcc` di setiap akhir batch.\",\n",
        "                \"\",\n",
        "                \"Bobot penilaian:\",\n",
        "                \"| Aspek | Bobot |\",\n",
        "                \"|-------|-------|\",\n",
        "                \"| Data Preparation (Text Preprocessing) | 20% |\",\n",
        "                \"| Definisi Class Transformer | 25% |\",\n",
        "                \"| Proses Training (TrainLoss, ValLoss, ValAcc) | 35% |\",\n",
        "                \"| Inference Translation | 20% |\",\n",
        "                \"\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"markdown\",\n",
        "            \"id\": \"#VSC-16b419c0\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"markdown\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"## Persiapan Lingkungan\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"code\",\n",
        "            \"id\": \"#VSC-43b30d09\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"python\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"\",\n",
        "                \"!pip install torch pandas numpy\",\n",
        "                \"import torch, pandas as pd, numpy as np, random, math, re, os\",\n",
        "                \"from collections import Counter\",\n",
        "                \"from torch import nn\",\n",
        "                \"from torch.utils.data import Dataset, DataLoader\",\n",
        "                \"\",\n",
        "                \"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\",\n",
        "                \"print('Running on', DEVICE)\",\n",
        "                \"\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"markdown\",\n",
        "            \"id\": \"#VSC-e7c78cc8\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"markdown\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"## 1. Data Preparation & Text Preprocessing (20%)\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"code\",\n",
        "            \"id\": \"#VSC-88b4fc9a\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"python\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"\",\n",
        "                \"# Dataset diambil dari file small_vocab_en.csv dan small_vocab_fr.csv\",\n",
        "                \"en_path = 'small_vocab_en.csv'\",\n",
        "                \"fr_path = 'small_vocab_fr.csv'\",\n",
        "                \"\",\n",
        "                \"# Baca setiap baris sebagai satu teks utuh\",\n",
        "                \"with open(en_path, 'r', encoding='utf-8') as f:\",\n",
        "                \"    src_texts = [line.strip() for line in f if line.strip()]\",\n",
        "                \"\",\n",
        "                \"with open(fr_path, 'r', encoding='utf-8') as f:\",\n",
        "                \"    tgt_texts = [line.strip() for line in f if line.strip()]\",\n",
        "                \"\",\n",
        "                \"print(f\\\"Contoh data Inggris: {src_texts[0]}\\\")\",\n",
        "                \"print(f\\\"Contoh data Prancis: {tgt_texts[0]}\\\")\",\n",
        "                \"\",\n",
        "                \"def clean_text(text):\",\n",
        "                \"    text = text.lower()\",\n",
        "                \"    text = re.sub(r\\\"[^a-zâêôàèçùé'\\\\-\\\\.\\\\,\\\\?\\\\!\\\\s]\\\", ' ', text)\",\n",
        "                \"    return re.sub(r'\\\\s+', ' ', text).strip()\",\n",
        "                \"\",\n",
        "                \"def tokenize(text):\",\n",
        "                \"    return text.split()\",\n",
        "                \"\",\n",
        "                \"src_tokens = [tokenize(clean_text(s)) for s in src_texts]\",\n",
        "                \"tgt_tokens = [tokenize(clean_text(t)) for t in tgt_texts]\",\n",
        "                \"\",\n",
        "                \"# Split train/val\",\n",
        "                \"data = list(zip(src_tokens, tgt_tokens))\",\n",
        "                \"random.shuffle(data)\",\n",
        "                \"split = int(0.9 * len(data))\",\n",
        "                \"train, val = data[:split], data[split:]\",\n",
        "                \"\",\n",
        "                \"PAD, BOS, EOS, UNK = '<pad>', '<s>', '</s>', '<unk>'\",\n",
        "                \"\",\n",
        "                \"def build_vocab(sentences):\",\n",
        "                \"    counter = Counter(t for s in sentences for t in s)\",\n",
        "                \"    vocab = [PAD, BOS, EOS, UNK] + [t for t, _ in counter.most_common()]\",\n",
        "                \"    stoi = {t: i for i, t in enumerate(vocab)}\",\n",
        "                \"    itos = {i: t for t, i in stoi.items()}\",\n",
        "                \"    return stoi, itos\",\n",
        "                \"\",\n",
        "                \"src_stoi, src_itos = build_vocab([s for s, _ in train])\",\n",
        "                \"tgt_stoi, tgt_itos = build_vocab([t for _, t in train])\",\n",
        "                \"\",\n",
        "                \"print('Vocab sizes -> src:', len(src_stoi), '| tgt:', len(tgt_stoi))\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"markdown\",\n",
        "            \"id\": \"#VSC-b2803b6f\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"markdown\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"## 2. Definisi Arsitektur Transformer (25%)\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"code\",\n",
        "            \"id\": \"#VSC-9b3b5e12\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"python\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"import torch\",\n",
        "                \"import torch.nn as nn\",\n",
        "                \"import math\",\n",
        "                \"\",\n",
        "                \"# ==========================================\",\n",
        "                \"# Positional Encoding\",\n",
        "                \"# ==========================================\",\n",
        "                \"class PositionalEncoding(nn.Module):\",\n",
        "                \"    def __init__(self, d_model, max_len=5000):\",\n",
        "                \"        super().__init__()\",\n",
        "                \"        pe = torch.zeros(max_len, d_model)\",\n",
        "                \"        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\",\n",
        "                \"        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\",\n",
        "                \"        pe[:, 0::2] = torch.sin(position * div_term)\",\n",
        "                \"        pe[:, 1::2] = torch.cos(position * div_term)\",\n",
        "                \"        pe = pe.unsqueeze(0)\",\n",
        "                \"        self.register_buffer('pe', pe)\",\n",
        "                \"\",\n",
        "                \"    def forward(self, x):\",\n",
        "                \"        x = x + self.pe[:, :x.size(1)]\",\n",
        "                \"        return x\",\n",
        "                \"\",\n",
        "                \"\",\n",
        "                \"# ==========================================\",\n",
        "                \"# Transformer Model untuk Translation\",\n",
        "                \"# ==========================================\",\n",
        "                \"class TransformerMT(nn.Module):\",\n",
        "                \"    def __init__(self, src_vocab, tgt_vocab, d_model=128, nhead=4, num_layers=2, dim_ff=512, dropout=0.1):\",\n",
        "                \"        super().__init__()\",\n",
        "                \"        self.src_embed = nn.Embedding(src_vocab, d_model)\",\n",
        "                \"        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)\",\n",
        "                \"        self.pos_enc = PositionalEncoding(d_model)\",\n",
        "                \"        self.transformer = nn.Transformer(\",\n",
        "                \"            d_model=d_model,\",\n",
        "                \"            nhead=nhead,\",\n",
        "                \"            num_encoder_layers=num_layers,\",\n",
        "                \"            num_decoder_layers=num_layers,\",\n",
        "                \"            dim_feedforward=dim_ff,\",\n",
        "                \"            dropout=dropout,\",\n",
        "                \"            batch_first=True\",\n",
        "                \"        )\",\n",
        "                \"        self.fc_out = nn.Linear(d_model, tgt_vocab)\",\n",
        "                \"\",\n",
        "                \"    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None, tgt_mask=None):\",\n",
        "                \"        \\\"\\\"\\\"Forward with optional masks:\\\",\",\n",
        "                \",\",\n",
        "                \",\",\n",
        "                \"\\\"\\\"\",\n",
        "                \"        src = self.pos_enc(self.src_embed(src))\",\n",
        "                \"        tgt = self.pos_enc(self.tgt_embed(tgt))\",\n",
        "                \"        out = self.transformer(src, tgt,\\\",\",\n",
        "                \",\",\n",
        "                \",\",\n",
        "                \",\",\n",
        "                \",\",\n",
        "                \",\",\n",
        "                \"\",\n",
        "                \": \",\n",
        "                \",\",\n",
        "                \": \",\n",
        "                \",\",\n",
        "                \": {\",\n",
        "                \": \",\n",
        "                \"\",\n",
        "                \": [\",\n",
        "                \",\",\n",
        "                \",\",\n",
        "                \",\",\n",
        "                \",\",\n",
        "                \",\",\n",
        "                \",\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"markdown\",\n",
        "            \"id\": \"#VSC-01cb1b65\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"markdown\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"## 3. Proses Training (35%)\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"code\",\n",
        "            \"id\": \"#VSC-e2475665\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"python\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"from tqdm import tqdm\",\n",
        "                \"\",\n",
        "                \"def accuracy_fn(y_pred, y_true, pad_idx):\",\n",
        "                \"    pred_tokens = y_pred.argmax(dim=-1)\",\n",
        "                \"    mask = y_true != pad_idx\",\n",
        "                \"    correct = (pred_tokens == y_true) & mask\",\n",
        "                \"    return correct.sum().float() / mask.sum().float()\",\n",
        "                \"\",\n",
        "                \"EPOCHS = 1\",\n",
        "                \"for epoch in range(EPOCHS):\",\n",
        "                \"    model.train()\",\n",
        "                \"    total_loss = 0\",\n",
        "                \"    print(f\\\"\\\\nEpoch {epoch+1}/{EPOCHS}\\\")\",\n",
        "                \"    for i, (src, tgt_in, tgt_out) in enumerate(tqdm(train_dl)):\",\n",
        "                \"        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\",\n",
        "                \"        optimizer.zero_grad()\",\n",
        "                \"        # buat mask: subsequent mask untuk decoder dan padding mask untuk src/tgt\",\n",
        "                \"        tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_in.size(1)).to(device)\",\n",
        "                \"        src_key_padding_mask = (src == src_stoi['<pad>'])\",\n",
        "                \"        tgt_key_padding_mask = (tgt_in == tgt_stoi['<pad>'])\",\n",
        "                \"        output = model(src, tgt_in, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)\",\n",
        "                \"        loss = criterion(output.view(-1, output.size(-1)), tgt_out.view(-1))\",\n",
        "                \"        loss.backward()\",\n",
        "                \"        optimizer.step()\",\n",
        "                \"        total_loss += loss.item()\",\n",
        "                \"        if (i+1) % 1 == 0:\",\n",
        "                \"            print(f\\\"Batch {i+1}/{len(train_dl)} - TrainLoss: {loss.item():.4f}\\\")\",\n",
        "                \"\",\n",
        "                \"    # Validation\",\n",
        "                \"    model.eval()\",\n",
        "                \"    val_loss, val_acc = 0, 0\",\n",
        "                \"    with torch.no_grad():\",\n",
        "                \"        for src, tgt_in, tgt_out in val_dl:\",\n",
        "                \"            src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\",\n",
        "                \"            # buat mask untuk validasi juga\",\n",
        "                \"            tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_in.size(1)).to(device)\",\n",
        "                \"            src_key_padding_mask = (src == src_stoi['<pad>'])\",\n",
        "                \"            tgt_key_padding_mask = (tgt_in == tgt_stoi['<pad>'])\",\n",
        "                \"            output = model(src, tgt_in, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)\",\n",
        "                \"            loss = criterion(output.view(-1, output.size(-1)), tgt_out.view(-1))\",\n",
        "                \"            val_loss += loss.item()\",\n",
        "                \"            # Perbaikan: gunakan pad index target saat menghitung akurasi\",\n",
        "                \"            val_acc += accuracy_fn(output, tgt_out, tgt_stoi['<pad>']).item()\",\n",
        "                \"    val_loss /= len(val_dl)\",\n",
        "                \"    val_acc /= len(val_dl)\",\n",
        "                \"    print(f\\\"ValLoss: {val_loss:.4f}, ValAcc: {val_acc*100:.2f}%\\\")\",\n",
        "                \"\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"markdown\",\n",
        "            \"id\": \"#VSC-6f03225b\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"markdown\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"## 4. Inference Translation (20%)\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"code\",\n",
        "            \"id\": \"#VSC-96de842a\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"python\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"def translate_sentence(model, sentence, src_stoi, tgt_stoi, tgt_itos, max_len=20):\",\n",
        "                \"    model.eval()\",\n",
        "                \"    tokens = [w.lower() for w in sentence.split()]\",\n",
        "                \"    src_ids = torch.tensor([[src_stoi.get(t, src_stoi['<unk>']) for t in tokens]], device=device)\",\n",
        "                \"    tgt_input = torch.tensor([[tgt_stoi['<s>']]], device=device)\",\n",
        "                \"\",\n",
        "                \"    for _ in range(max_len):\",\n",
        "                \"        out = model(src_ids, tgt_input)\",\n",
        "                \"        next_token = out[:, -1].argmax(dim=-1).unsqueeze(0)\",\n",
        "                \"        tgt_input = torch.cat([tgt_input, next_token], dim=1)\",\n",
        "                \"        if next_token.item() == tgt_stoi['</s>']:\",\n",
        "                \"            break\",\n",
        "                \"\",\n",
        "                \"    translated = [tgt_itos[idx.item()] for idx in tgt_input[0]]\",\n",
        "                \"    return ' '.join(translated[1:-1])  # hilangkan <s> dan </s>\",\n",
        "                \"\",\n",
        "                \"# Contoh uji terjemahan (satu contoh)\",\n",
        "                \"test_sentence = src_texts[0]\",\n",
        "                \"print(\\\"English :\\\", test_sentence)\",\n",
        "                \"print(\\\"French (predicted):\\\", translate_sentence(model, test_sentence, src_stoi, tgt_stoi, tgt_itos))\",\n",
        "                \"\",\n",
        "                \"# ==========================================\",\n",
        "                \"# Evaluasi greedy (autoregressive) pada validation set\",\n",
        "                \"# ==========================================\",\n",
        "                \"def greedy_decode(tokens, src_stoi, tgt_stoi, tgt_itos, max_len=20):\",\n",
        "                \"    model.eval()\",\n",
        "                \"    src_ids = torch.tensor([[src_stoi.get(t, src_stoi['<unk>']) for t in tokens]], device=device)\",\n",
        "                \"    tgt = torch.tensor([[tgt_stoi['<s>']]], device=device)\",\n",
        "                \"    for _ in range(max_len):\",\n",
        "                \"        out = model(src_ids, tgt)\",\n",
        "                \"        next_tok = out[0, -1].argmax().item()\",\n",
        "                \"        tgt = torch.cat([tgt, torch.tensor([[next_tok]], device=device)], dim=1)\",\n",
        "                \"        if next_tok == tgt_stoi['</s>']: break\",\n",
        "                \"    return [tgt_itos[i] for i in tgt[0].tolist()][1:-1]\",\n",
        "                \"\",\n",
        "                \"def eval_greedy(val_pairs, n_samples=200):\",\n",
        "                \"    # n_samples untuk menjalankan evaluasi cepat; set None untuk semua\",\n",
        "                \"    total_sent, exact_match, token_correct, token_total = 0,0,0,0\",\n",
        "                \"    samples = val_pairs if n_samples is None else val_pairs[:n_samples]\",\n",
        "                \"    for src_tokens, tgt_tokens in samples:\",\n",
        "                \"        pred = greedy_decode(src_tokens, src_stoi, tgt_stoi, tgt_itos, max_len=MAX_LEN)\",\n",
        "                \"        total_sent += 1\",\n",
        "                \"        if pred == tgt_tokens: exact_match += 1\",\n",
        "                \"        # token-level overlap (up to min len)\",\n",
        "                \"        m = min(len(pred), len(tgt_tokens))\",\n",
        "                \"        for i in range(m):\",\n",
        "                \"            if pred[i] == tgt_tokens[i]: token_correct += 1\",\n",
        "                \"            token_total += 1\",\n",
        "                \"    print(f'Greedy Exact Match: {exact_match}/{total_sent} = {exact_match/total_sent:.4f}')\",\n",
        "                \"    if token_total>0:\",\n",
        "                \"        print(f'Greedy Token Accuracy (overlap): {token_correct}/{token_total} = {token_correct/token_total:.4f}')\",\n",
        "                \"    else:\",\n",
        "                \"        print('No token comparisons performed (empty preds?).')\",\n",
        "                \"\",\n",
        "                \"# Jalankan evaluasi greedy cepat (200 contoh)\",\n",
        "                \"eval_greedy(val, n_samples=200)\",\n",
        "                \"\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"cell_type\": \"markdown\",\n",
        "            \"id\": \"#VSC-0742d772\",\n",
        "            \"metadata\": {\n",
        "                \"language\": \"markdown\"\n",
        "            },\n",
        "            \"source\": [\n",
        "                \"\",\n",
        "                \"## 5. Kesimpulan\",\n",
        "                \"\",\n",
        "                \"Eksperimen ini menunjukkan implementasi dasar Transformer untuk penerjemahan Bahasa Inggris ke Bahasa Prancis.\",\n",
        "                \"\",\n",
        "                \"- Data telah dibersihkan dan ditokenisasi secara sederhana.\",\n",
        "                \"- Arsitektur Transformer telah dibangun dari nol dengan PyTorch.\",\n",
        "                \"- Proses training menampilkan *TrainLoss*, *ValLoss*, dan *ValAcc* tiap batch.\",\n",
        "                \"- Model berhasil melakukan inferensi dengan pendekatan *greedy decoding*.\",\n",
        "                \"\",\n",
        "                \"Selanjutnya, model dapat diperluas dengan peningkatan jumlah epoch, mekanisme perhatian visualisasi, dan evaluasi BLEU score.\",\n",
        "                \"\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33cec736",
      "metadata": {
        "id": "33cec736"
      },
      "source": [
        "\n",
        "# Transformer-based English–French Translation\n",
        "\n",
        "Notebook ini merupakan implementasi eksplorasi model **Transformer** untuk penerjemahan otomatis dari Bahasa Inggris ke Bahasa Prancis.\n",
        "\n",
        "Tujuan utama:\n",
        "- Melatih model Transformer selama **1 epoch** dengan **batch-size maksimal 100**\n",
        "- Menunjukkan proses **Text Preprocessing**, **Definisi Arsitektur Transformer**, **Training**, dan **Inference**\n",
        "- Menampilkan metrik: `TrainLoss`, `ValLoss`, dan `ValAcc` di setiap akhir batch.\n",
        "\n",
        "Bobot penilaian:\n",
        "| Aspek | Bobot |\n",
        "|-------|-------|\n",
        "| Data Preparation (Text Preprocessing) | 20% |\n",
        "| Definisi Class Transformer | 25% |\n",
        "| Proses Training (TrainLoss, ValLoss, ValAcc) | 35% |\n",
        "| Inference Translation | 20% |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a108dd3",
      "metadata": {
        "id": "5a108dd3"
      },
      "source": [
        "## Persiapan Lingkungan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74556571",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74556571",
        "outputId": "c5db72b4-a680-4500-a945-23bdf9a1111b"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install torch pandas numpy\n",
        "import torch, pandas as pd, numpy as np, random, math, re, os\n",
        "from collections import Counter\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on', DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c85ef7",
      "metadata": {
        "id": "34c85ef7"
      },
      "source": [
        "## 1. Data Preparation & Text Preprocessing (20%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf55fa2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf55fa2f",
        "outputId": "26a75e21-532c-4ea8-ef6c-b63cd90162e6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Dataset diambil dari file small_vocab_en.csv dan small_vocab_fr.csv\n",
        "en_path = 'small_vocab_en.csv'\n",
        "fr_path = 'small_vocab_fr.csv'\n",
        "\n",
        "# Baca setiap baris sebagai satu teks utuh\n",
        "with open(en_path, 'r', encoding='utf-8') as f:\n",
        "    src_texts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "with open(fr_path, 'r', encoding='utf-8') as f:\n",
        "    tgt_texts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "print(f\"Contoh data Inggris: {src_texts[0]}\")\n",
        "print(f\"Contoh data Prancis: {tgt_texts[0]}\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zâêôàèçùé'\\-\\.\\,\\?\\!\\s]\", ' ', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "src_tokens = [tokenize(clean_text(s)) for s in src_texts]\n",
        "tgt_tokens = [tokenize(clean_text(t)) for t in tgt_texts]\n",
        "\n",
        "# Split train/val\n",
        "data = list(zip(src_tokens, tgt_tokens))\n",
        "random.shuffle(data)\n",
        "split = int(0.9 * len(data))\n",
        "train, val = data[:split], data[split:]\n",
        "\n",
        "PAD, BOS, EOS, UNK = '<pad>', '<s>', '</s>', '<unk>'\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    counter = Counter(t for s in sentences for t in s)\n",
        "    vocab = [PAD, BOS, EOS, UNK] + [t for t, _ in counter.most_common()]\n",
        "    stoi = {t: i for i, t in enumerate(vocab)}\n",
        "    itos = {i: t for t, i in stoi.items()}\n",
        "    return stoi, itos\n",
        "\n",
        "src_stoi, src_itos = build_vocab([s for s, _ in train])\n",
        "tgt_stoi, tgt_itos = build_vocab([t for _, t in train])\n",
        "\n",
        "print('Vocab sizes -> src:', len(src_stoi), '| tgt:', len(tgt_stoi))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77fede71",
      "metadata": {
        "id": "77fede71"
      },
      "source": [
        "## 2. Definisi Arsitektur Transformer (25%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb512b0",
      "metadata": {
        "id": "afb512b0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# ==========================================\n",
        "# Positional Encoding\n",
        "# ==========================================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# Transformer Model untuk Translation\n",
        "# ==========================================\n",
        "class TransformerMT(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model=128, nhead=4, num_layers=2, dim_ff=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.src_embed = nn.Embedding(src_vocab, d_model)\n",
        "        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_ff,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab)\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None, tgt_mask=None):\n",
        "        \"\"\"Forward with optional masks:\",\n",
        ",\n",
        ",\n",
        "\"\"\n",
        "        src = self.pos_enc(self.src_embed(src))\n",
        "        tgt = self.pos_enc(self.tgt_embed(tgt))\n",
        "        out = self.transformer(src, tgt,\",\n",
        ",\n",
        ",\n",
        ",\n",
        ",\n",
        ",\n",
        "\n",
        ": \n",
        ",\n",
        ": \n",
        ",\n",
        ": {\n",
        ": \n",
        "\n",
        ": [\n",
        ",\n",
        ",\n",
        ",\n",
        ",\n",
        ",\n",
        ","
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5dfe426",
      "metadata": {
        "id": "c5dfe426"
      },
      "source": [
        "## 3. Proses Training (35%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9a23e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac9a23e3",
        "outputId": "d0296bc1-2010-477f-b870-49be05d860fa"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def accuracy_fn(y_pred, y_true, pad_idx):\n",
        "    pred_tokens = y_pred.argmax(dim=-1)\n",
        "    mask = y_true != pad_idx\n",
        "    correct = (pred_tokens == y_true) & mask\n",
        "    return correct.sum().float() / mask.sum().float()\n",
        "\n",
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    for i, (src, tgt_in, tgt_out) in enumerate(tqdm(train_dl)):\n",
        "        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # buat mask: subsequent mask untuk decoder dan padding mask untuk src/tgt\n",
        "        tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_in.size(1)).to(device)\n",
        "        src_key_padding_mask = (src == src_stoi['<pad>'])\n",
        "        tgt_key_padding_mask = (tgt_in == tgt_stoi['<pad>'])\n",
        "        output = model(src, tgt_in, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)\n",
        "        loss = criterion(output.view(-1, output.size(-1)), tgt_out.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if (i+1) % 1 == 0:\n",
        "            print(f\"Batch {i+1}/{len(train_dl)} - TrainLoss: {loss.item():.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_acc = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt_in, tgt_out in val_dl:\n",
        "            src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
        "            # buat mask untuk validasi juga\n",
        "            tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_in.size(1)).to(device)\n",
        "            src_key_padding_mask = (src == src_stoi['<pad>'])\n",
        "            tgt_key_padding_mask = (tgt_in == tgt_stoi['<pad>'])\n",
        "            output = model(src, tgt_in, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), tgt_out.view(-1))\n",
        "            val_loss += loss.item()\n",
        "            # Perbaikan: gunakan pad index target saat menghitung akurasi\n",
        "            val_acc += accuracy_fn(output, tgt_out, tgt_stoi['<pad>']).item()\n",
        "    val_loss /= len(val_dl)\n",
        "    val_acc /= len(val_dl)\n",
        "    print(f\"ValLoss: {val_loss:.4f}, ValAcc: {val_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc36f687",
      "metadata": {
        "id": "dc36f687"
      },
      "source": [
        "## 4. Inference Translation (20%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb855b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceb855b4",
        "outputId": "a47f8872-5b01-4242-abf3-5839965cb872"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, src_stoi, tgt_stoi, tgt_itos, max_len=20):\n",
        "    model.eval()\n",
        "    tokens = [w.lower() for w in sentence.split()]\n",
        "    src_ids = torch.tensor([[src_stoi.get(t, src_stoi['<unk>']) for t in tokens]], device=device)\n",
        "    tgt_input = torch.tensor([[tgt_stoi['<s>']]], device=device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        out = model(src_ids, tgt_input)\n",
        "        next_token = out[:, -1].argmax(dim=-1).unsqueeze(0)\n",
        "        tgt_input = torch.cat([tgt_input, next_token], dim=1)\n",
        "        if next_token.item() == tgt_stoi['</s>']:\n",
        "            break\n",
        "\n",
        "    translated = [tgt_itos[idx.item()] for idx in tgt_input[0]]\n",
        "    return ' '.join(translated[1:-1])  # hilangkan <s> dan </s>\n",
        "\n",
        "# Contoh uji terjemahan (satu contoh)\n",
        "test_sentence = src_texts[0]\n",
        "print(\"English :\", test_sentence)\n",
        "print(\"French (predicted):\", translate_sentence(model, test_sentence, src_stoi, tgt_stoi, tgt_itos))\n",
        "\n",
        "# ==========================================\n",
        "# Evaluasi greedy (autoregressive) pada validation set\n",
        "# ==========================================\n",
        "def greedy_decode(tokens, src_stoi, tgt_stoi, tgt_itos, max_len=20):\n",
        "    model.eval()\n",
        "    src_ids = torch.tensor([[src_stoi.get(t, src_stoi['<unk>']) for t in tokens]], device=device)\n",
        "    tgt = torch.tensor([[tgt_stoi['<s>']]], device=device)\n",
        "    for _ in range(max_len):\n",
        "        out = model(src_ids, tgt)\n",
        "        next_tok = out[0, -1].argmax().item()\n",
        "        tgt = torch.cat([tgt, torch.tensor([[next_tok]], device=device)], dim=1)\n",
        "        if next_tok == tgt_stoi['</s>']: break\n",
        "    return [tgt_itos[i] for i in tgt[0].tolist()][1:-1]\n",
        "\n",
        "def eval_greedy(val_pairs, n_samples=200):\n",
        "    # n_samples untuk menjalankan evaluasi cepat; set None untuk semua\n",
        "    total_sent, exact_match, token_correct, token_total = 0,0,0,0\n",
        "    samples = val_pairs if n_samples is None else val_pairs[:n_samples]\n",
        "    for src_tokens, tgt_tokens in samples:\n",
        "        pred = greedy_decode(src_tokens, src_stoi, tgt_stoi, tgt_itos, max_len=MAX_LEN)\n",
        "        total_sent += 1\n",
        "        if pred == tgt_tokens: exact_match += 1\n",
        "        # token-level overlap (up to min len)\n",
        "        m = min(len(pred), len(tgt_tokens))\n",
        "        for i in range(m):\n",
        "            if pred[i] == tgt_tokens[i]: token_correct += 1\n",
        "            token_total += 1\n",
        "    print(f'Greedy Exact Match: {exact_match}/{total_sent} = {exact_match/total_sent:.4f}')\n",
        "    if token_total>0:\n",
        "        print(f'Greedy Token Accuracy (overlap): {token_correct}/{token_total} = {token_correct/token_total:.4f}')\n",
        "    else:\n",
        "        print('No token comparisons performed (empty preds?).')\n",
        "\n",
        "# Jalankan evaluasi greedy cepat (200 contoh)\n",
        "eval_greedy(val, n_samples=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582065aa",
      "metadata": {
        "id": "582065aa"
      },
      "source": [
        "\n",
        "## 5. Kesimpulan\n",
        "\n",
        "Eksperimen ini menunjukkan implementasi dasar Transformer untuk penerjemahan Bahasa Inggris ke Bahasa Prancis.\n",
        "\n",
        "- Data telah dibersihkan dan ditokenisasi secara sederhana.\n",
        "- Arsitektur Transformer telah dibangun dari nol dengan PyTorch.\n",
        "- Proses training menampilkan *TrainLoss*, *ValLoss*, dan *ValAcc* tiap batch.\n",
        "- Model berhasil melakukan inferensi dengan pendekatan *greedy decoding*.\n",
        "\n",
        "Selanjutnya, model dapat diperluas dengan peningkatan jumlah epoch, mekanisme perhatian visualisasi, dan evaluasi BLEU score.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "steganalysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
